{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoireCNN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (deconv1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (upsample): Upsample(size=(256, 256), mode='bilinear')\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your MoireCNN model class as provided\n",
    "class MoireCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoireCNN, self).__init__()\n",
    "        \n",
    "        # Encoder part (downsampling)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Decoder part (upsampling)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # Adjusted kernel size\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Adjusted kernel size\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)    # Adjusted kernel size\n",
    "\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Upsampling layer to scale up the final output to 256x256\n",
    "        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        \n",
    "        # Decoder path (upsample)\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.deconv3(x)  # Final output layer to match 3 channels\n",
    "        \n",
    "        # Upsample to 256x256\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Load the model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model and load saved weights\n",
    "model = MoireCNN().to(device)\n",
    "model.load_state_dict(torch.load('moire_model.pth', map_location=device))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256 since your model uses this size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization\n",
    "])\n",
    "\n",
    "# Function to predict if an image contains moiré or not\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure the image is in RGB mode\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Run the image through the model\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 0000_gt.jpg...\n",
      "Image 0000_gt.jpg is classified as: No Moiré\n",
      "Classifying 0000_moire.jpg...\n",
      "Image 0000_moire.jpg is classified as: Moiré\n",
      "Classifying 0001_gt.jpg...\n",
      "Image 0001_gt.jpg is classified as: Moiré\n",
      "Classifying 0001_moire.jpg...\n",
      "Image 0001_moire.jpg is classified as: Moiré\n",
      "Classifying 0002_gt.jpg...\n",
      "Image 0002_gt.jpg is classified as: No Moiré\n",
      "Classifying 0002_moire.jpg...\n",
      "Image 0002_moire.jpg is classified as: Moiré\n",
      "Classifying 0004_gt.jpg...\n",
      "Image 0004_gt.jpg is classified as: Moiré\n",
      "Classifying 0004_moire.jpg...\n",
      "Image 0004_moire.jpg is classified as: Moiré\n",
      "Classifying 0005_gt.jpg...\n",
      "Image 0005_gt.jpg is classified as: Moiré\n",
      "Classifying 0005_moire.jpg...\n",
      "Image 0005_moire.jpg is classified as: Moiré\n",
      "Classifying 0006_gt.jpg...\n",
      "Image 0006_gt.jpg is classified as: Moiré\n",
      "Classifying 0006_moire.jpg...\n",
      "Image 0006_moire.jpg is classified as: Moiré\n",
      "Classifying 0007_gt.jpg...\n",
      "Image 0007_gt.jpg is classified as: Moiré\n",
      "Classifying 0007_moire.jpg...\n",
      "Image 0007_moire.jpg is classified as: Moiré\n",
      "Classifying 0009_gt.jpg...\n",
      "Image 0009_gt.jpg is classified as: Moiré\n",
      "Classifying 0009_moire.jpg...\n",
      "Image 0009_moire.jpg is classified as: Moiré\n",
      "Classifying 0010_gt.jpg...\n",
      "Image 0010_gt.jpg is classified as: Moiré\n",
      "Classifying 0010_moire.jpg...\n",
      "Image 0010_moire.jpg is classified as: Moiré\n",
      "Classifying 0011_gt.jpg...\n",
      "Image 0011_gt.jpg is classified as: Moiré\n",
      "Classifying 0011_moire.jpg...\n",
      "Image 0011_moire.jpg is classified as: Moiré\n",
      "Classifying 0012_gt.jpg...\n",
      "Image 0012_gt.jpg is classified as: Moiré\n",
      "Classifying 0012_moire.jpg...\n",
      "Image 0012_moire.jpg is classified as: Moiré\n",
      "Classifying 0013_gt.jpg...\n",
      "Image 0013_gt.jpg is classified as: Moiré\n",
      "Classifying 0013_moire.jpg...\n",
      "Image 0013_moire.jpg is classified as: Moiré\n",
      "Classifying 0014_gt.jpg...\n",
      "Image 0014_gt.jpg is classified as: No Moiré\n",
      "Classifying 0014_moire.jpg...\n",
      "Image 0014_moire.jpg is classified as: Moiré\n",
      "Classifying 0017_gt.jpg...\n",
      "Image 0017_gt.jpg is classified as: Moiré\n",
      "Classifying 0017_moire.jpg...\n",
      "Image 0017_moire.jpg is classified as: Moiré\n",
      "Classifying 0018_gt.jpg...\n",
      "Image 0018_gt.jpg is classified as: Moiré\n",
      "Classifying 0018_moire.jpg...\n",
      "Image 0018_moire.jpg is classified as: Moiré\n",
      "Classifying 0020_gt.jpg...\n",
      "Image 0020_gt.jpg is classified as: Moiré\n",
      "Classifying 0020_moire.jpg...\n",
      "Image 0020_moire.jpg is classified as: Moiré\n",
      "Classifying 0021_gt.jpg...\n",
      "Image 0021_gt.jpg is classified as: Moiré\n",
      "Classifying 0021_moire.jpg...\n",
      "Image 0021_moire.jpg is classified as: Moiré\n",
      "Classifying 0022_gt.jpg...\n",
      "Image 0022_gt.jpg is classified as: Moiré\n",
      "Classifying 0022_moire.jpg...\n",
      "Image 0022_moire.jpg is classified as: Moiré\n",
      "Classifying 0023_gt.jpg...\n",
      "Image 0023_gt.jpg is classified as: Moiré\n",
      "Classifying 0023_moire.jpg...\n",
      "Image 0023_moire.jpg is classified as: Moiré\n",
      "Classifying 0024_gt.jpg...\n",
      "Image 0024_gt.jpg is classified as: Moiré\n",
      "Classifying 0024_moire.jpg...\n",
      "Image 0024_moire.jpg is classified as: Moiré\n",
      "Classifying 0025_gt.jpg...\n",
      "Image 0025_gt.jpg is classified as: Moiré\n",
      "Classifying 0025_moire.jpg...\n",
      "Image 0025_moire.jpg is classified as: Moiré\n",
      "Classifying 0026_gt.jpg...\n",
      "Image 0026_gt.jpg is classified as: Moiré\n",
      "Classifying 0026_moire.jpg...\n",
      "Image 0026_moire.jpg is classified as: Moiré\n",
      "Classifying 0027_gt.jpg...\n",
      "Image 0027_gt.jpg is classified as: Moiré\n",
      "Classifying 0027_moire.jpg...\n",
      "Image 0027_moire.jpg is classified as: Moiré\n",
      "Classifying 0028_gt.jpg...\n",
      "Image 0028_gt.jpg is classified as: Moiré\n",
      "Classifying 0028_moire.jpg...\n",
      "Image 0028_moire.jpg is classified as: Moiré\n",
      "Classifying 0030_gt.jpg...\n",
      "Image 0030_gt.jpg is classified as: Moiré\n",
      "Classifying 0030_moire.jpg...\n",
      "Image 0030_moire.jpg is classified as: Moiré\n",
      "Classifying 0031_gt.jpg...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is classified as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage: Replace 'your_images_folder' with the folder containing images\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m classify_images_in_folder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset/train/train/pair_00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[50], line 34\u001b[0m, in \u001b[0;36mclassify_images_in_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m folder\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Change to *.png or any format you have\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     output \u001b[38;5;241m=\u001b[39m predict_image(img_path)\n\u001b[0;32m     35\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m classify_moire(output)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is classified as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 13\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Add batch dimension and move to device\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(image)  \u001b[38;5;66;03m# Run the image through the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[48], line 38\u001b[0m, in \u001b[0;36mMoireCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv1(x))\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv2(x))\n\u001b[1;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv3(x)  \u001b[38;5;66;03m# Final output layer to match 3 channels\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Upsample to 256x256\u001b[39;00m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:1162\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m   1151\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1152\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1154\u001b[0m     output_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv_transpose2d(\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m   1168\u001b[0m     output_padding,\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,\n\u001b[0;32m   1171\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to classify whether moiré is present or not\n",
    "def classify_moire(output):\n",
    "    # You can use a threshold here to classify. Let's assume:\n",
    "    # If the max value in the output is greater than a certain threshold, we classify it as moiré.\n",
    "    threshold = 1.5  # Set a threshold, adjust as needed\n",
    "    output_max = torch.max(output).item()  # Get the maximum value of the output\n",
    "\n",
    "    if output_max > threshold:\n",
    "        return 'Moiré'\n",
    "    else:\n",
    "        return 'No Moiré'\n",
    "\n",
    "def visualize_prediction(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Get the output of the model\n",
    "    output = predict_image(image_path)\n",
    "    \n",
    "    # Classify the output\n",
    "    prediction = classify_moire(output)\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def classify_images_in_folder(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    for img_path in folder.glob('*.jpg'):  # Change to *.png or any format you have\n",
    "        print(f\"Classifying {img_path.name}...\")\n",
    "        output = predict_image(img_path)\n",
    "        prediction = classify_moire(output)\n",
    "        print(f\"Image {img_path.name} is classified as: {prediction}\")\n",
    "\n",
    "# Example usage: Replace 'your_images_folder' with the folder containing images\n",
    "classify_images_in_folder('Dataset/train/train/pair_00')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
