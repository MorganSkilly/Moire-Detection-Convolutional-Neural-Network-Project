{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step guide to create a PyTorch custom dataset to train a model using two groups of images: moire present and clean. The images are stored in the same folder, with filenames for moiré images ending in _moire.jpg and their clean counterparts ending in _gt.jpg. The images have a resolution of 4032x3024. The guide will also include GPU support for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Required Libraries\n",
    "You need to start by importing the necessary libraries for data processing, model creation, and GPU handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm  # Optional, for progress bars\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating the Custom Dataset Class\n",
    "We will create a custom dataset class that loads the images and assigns labels. In this case:\n",
    "\n",
    "Images with '_moire.jpg' will be assigned label 1 (moire present).\n",
    "Images with '_gt.jpg' will be assigned label 0 (clean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoireCleanDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory containing all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # List all images in the directory\n",
    "        self.images = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "\n",
    "        # Open the image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Label: 1 for moire images (those with '_moire' in the filename), 0 for clean images\n",
    "        if '_moire' in img_name:\n",
    "            label = 1  # Moire image\n",
    "        else:\n",
    "            label = 0  # Clean image\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "root_dir: The folder where your images are stored.\n",
    "images: A list of filenames ending with _moire.jpg.\n",
    "The getitem function returns the pair of images: the moire image and its corresponding clean version, along with the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transformations\n",
    "In order to preprocess the images, you can apply transformations such as resizing, normalization, and conversion to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256 (you can change this)\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Resize((256, 256)) ensures that the images are resized to a manageable size. You can change this to the resolution you prefer. If you need to work with the original size of 4032x3024, you can adjust the Resize() transform accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creating Dataset and DataLoader\n",
    "Now that we’ve defined the dataset class, let's instantiate it and load the data using the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moire image: 0000_gt.jpg\n",
      "Moire image: 0000_moire.jpg\n",
      "Moire image: 0001_gt.jpg\n",
      "Moire image: 0001_moire.jpg\n",
      "Moire image: 0002_gt.jpg\n",
      "Moire image: 0002_moire.jpg\n",
      "Moire image: 0004_gt.jpg\n",
      "Moire image: 0004_moire.jpg\n",
      "Moire image: 0005_gt.jpg\n",
      "Moire image: 0005_moire.jpg\n",
      "Moire image: 0006_gt.jpg\n",
      "Moire image: 0006_moire.jpg\n",
      "Moire image: 0007_gt.jpg\n",
      "Moire image: 0007_moire.jpg\n",
      "Moire image: 0009_gt.jpg\n",
      "Moire image: 0009_moire.jpg\n",
      "Moire image: 0010_gt.jpg\n",
      "Moire image: 0010_moire.jpg\n",
      "Moire image: 0011_gt.jpg\n",
      "Moire image: 0011_moire.jpg\n",
      "Moire image: 0012_gt.jpg\n",
      "Moire image: 0012_moire.jpg\n",
      "Moire image: 0013_gt.jpg\n",
      "Moire image: 0013_moire.jpg\n",
      "Moire image: 0014_gt.jpg\n",
      "Moire image: 0014_moire.jpg\n",
      "Moire image: 0017_gt.jpg\n",
      "Moire image: 0017_moire.jpg\n",
      "Moire image: 0018_gt.jpg\n",
      "Moire image: 0018_moire.jpg\n",
      "Moire image: 0020_gt.jpg\n",
      "Moire image: 0020_moire.jpg\n",
      "Moire image: 0021_gt.jpg\n",
      "Moire image: 0021_moire.jpg\n",
      "Moire image: 0022_gt.jpg\n",
      "Moire image: 0022_moire.jpg\n",
      "Moire image: 0023_gt.jpg\n",
      "Moire image: 0023_moire.jpg\n",
      "Moire image: 0024_gt.jpg\n",
      "Moire image: 0024_moire.jpg\n",
      "Moire image: 0025_gt.jpg\n",
      "Moire image: 0025_moire.jpg\n",
      "Moire image: 0026_gt.jpg\n",
      "Moire image: 0026_moire.jpg\n",
      "Moire image: 0027_gt.jpg\n",
      "Moire image: 0027_moire.jpg\n",
      "Moire image: 0028_gt.jpg\n",
      "Moire image: 0028_moire.jpg\n",
      "Moire image: 0030_gt.jpg\n",
      "Moire image: 0030_moire.jpg\n",
      "Moire image: 0031_gt.jpg\n",
      "Moire image: 0031_moire.jpg\n",
      "Moire image: 0033_gt.jpg\n",
      "Moire image: 0033_moire.jpg\n",
      "Moire image: 0034_gt.jpg\n",
      "Moire image: 0034_moire.jpg\n",
      "Moire image: 0035_gt.jpg\n",
      "Moire image: 0035_moire.jpg\n",
      "Moire image: 0036_gt.jpg\n",
      "Moire image: 0036_moire.jpg\n",
      "Moire image: 0037_gt.jpg\n",
      "Moire image: 0037_moire.jpg\n",
      "Moire image: 0038_gt.jpg\n",
      "Moire image: 0038_moire.jpg\n",
      "Moire image: 0039_gt.jpg\n",
      "Moire image: 0039_moire.jpg\n",
      "Moire image: 0040_gt.jpg\n",
      "Moire image: 0040_moire.jpg\n",
      "Moire image: 0041_gt.jpg\n",
      "Moire image: 0041_moire.jpg\n",
      "Moire image: 0043_gt.jpg\n",
      "Moire image: 0043_moire.jpg\n",
      "Moire image: 0044_gt.jpg\n",
      "Moire image: 0044_moire.jpg\n",
      "Moire image: 0046_gt.jpg\n",
      "Moire image: 0046_moire.jpg\n",
      "Moire image: 0047_gt.jpg\n",
      "Moire image: 0047_moire.jpg\n",
      "Moire image: 0048_gt.jpg\n",
      "Moire image: 0048_moire.jpg\n",
      "Moire image: 0049_gt.jpg\n",
      "Moire image: 0049_moire.jpg\n",
      "Moire image: 0050_gt.jpg\n",
      "Moire image: 0050_moire.jpg\n",
      "Moire image: 0051_gt.jpg\n",
      "Moire image: 0051_moire.jpg\n",
      "Moire image: 0052_gt.jpg\n",
      "Moire image: 0052_moire.jpg\n",
      "Moire image: 0053_gt.jpg\n",
      "Moire image: 0053_moire.jpg\n",
      "Moire image: 0054_gt.jpg\n",
      "Moire image: 0054_moire.jpg\n",
      "Moire image: 0055_gt.jpg\n",
      "Moire image: 0055_moire.jpg\n",
      "Moire image: 0056_gt.jpg\n",
      "Moire image: 0056_moire.jpg\n",
      "Moire image: 0058_gt.jpg\n",
      "Moire image: 0058_moire.jpg\n",
      "Moire image: 0059_gt.jpg\n",
      "Moire image: 0059_moire.jpg\n",
      "Moire image: 0060_gt.jpg\n",
      "Moire image: 0060_moire.jpg\n",
      "Moire image: 0062_gt.jpg\n",
      "Moire image: 0062_moire.jpg\n",
      "Moire image: 0063_gt.jpg\n",
      "Moire image: 0063_moire.jpg\n",
      "Moire image: 0064_gt.jpg\n",
      "Moire image: 0064_moire.jpg\n",
      "Moire image: 0066_gt.jpg\n",
      "Moire image: 0066_moire.jpg\n",
      "Moire image: 0067_gt.jpg\n",
      "Moire image: 0067_moire.jpg\n",
      "Moire image: 0068_gt.jpg\n",
      "Moire image: 0068_moire.jpg\n",
      "Moire image: 0069_gt.jpg\n",
      "Moire image: 0069_moire.jpg\n",
      "Moire image: 0070_gt.jpg\n",
      "Moire image: 0070_moire.jpg\n",
      "Moire image: 0071_gt.jpg\n",
      "Moire image: 0071_moire.jpg\n",
      "Moire image: 0072_gt.jpg\n",
      "Moire image: 0072_moire.jpg\n",
      "Moire image: 0073_gt.jpg\n",
      "Moire image: 0073_moire.jpg\n",
      "Moire image: 0074_gt.jpg\n",
      "Moire image: 0074_moire.jpg\n",
      "Moire image: 0075_gt.jpg\n",
      "Moire image: 0075_moire.jpg\n",
      "Moire image: 0076_gt.jpg\n",
      "Moire image: 0076_moire.jpg\n",
      "Moire image: 0077_gt.jpg\n",
      "Moire image: 0077_moire.jpg\n",
      "Moire image: 0078_gt.jpg\n",
      "Moire image: 0078_moire.jpg\n",
      "Moire image: 0079_gt.jpg\n",
      "Moire image: 0079_moire.jpg\n",
      "Moire image: 0081_gt.jpg\n",
      "Moire image: 0081_moire.jpg\n",
      "Moire image: 0082_gt.jpg\n",
      "Moire image: 0082_moire.jpg\n",
      "Moire image: 0083_gt.jpg\n",
      "Moire image: 0083_moire.jpg\n",
      "Moire image: 0084_gt.jpg\n",
      "Moire image: 0084_moire.jpg\n",
      "Moire image: 0085_gt.jpg\n",
      "Moire image: 0085_moire.jpg\n",
      "Moire image: 0086_gt.jpg\n",
      "Moire image: 0086_moire.jpg\n",
      "Moire image: 0087_gt.jpg\n",
      "Moire image: 0087_moire.jpg\n",
      "Moire image: 0088_gt.jpg\n",
      "Moire image: 0088_moire.jpg\n",
      "Moire image: 0089_gt.jpg\n",
      "Moire image: 0089_moire.jpg\n",
      "Moire image: 0090_gt.jpg\n",
      "Moire image: 0090_moire.jpg\n",
      "Moire image: 0091_gt.jpg\n",
      "Moire image: 0091_moire.jpg\n",
      "Moire image: 0092_gt.jpg\n",
      "Moire image: 0092_moire.jpg\n",
      "Moire image: 0093_gt.jpg\n",
      "Moire image: 0093_moire.jpg\n",
      "Moire image: 0095_gt.jpg\n",
      "Moire image: 0095_moire.jpg\n",
      "Moire image: 0096_gt.jpg\n",
      "Moire image: 0096_moire.jpg\n",
      "Moire image: 0097_gt.jpg\n",
      "Moire image: 0097_moire.jpg\n",
      "Moire image: 0098_gt.jpg\n",
      "Moire image: 0098_moire.jpg\n",
      "Moire image: 0099_gt.jpg\n",
      "Moire image: 0099_moire.jpg\n",
      "Moire image: 0100_gt.jpg\n",
      "Moire image: 0100_moire.jpg\n",
      "Moire image: 0101_gt.jpg\n",
      "Moire image: 0101_moire.jpg\n",
      "Moire image: 0102_gt.jpg\n",
      "Moire image: 0102_moire.jpg\n",
      "Moire image: 0103_gt.jpg\n",
      "Moire image: 0103_moire.jpg\n",
      "Moire image: 0104_gt.jpg\n",
      "Moire image: 0104_moire.jpg\n",
      "Moire image: 0105_gt.jpg\n",
      "Moire image: 0105_moire.jpg\n",
      "Moire image: 0106_gt.jpg\n",
      "Moire image: 0106_moire.jpg\n",
      "Moire image: 0107_gt.jpg\n",
      "Moire image: 0107_moire.jpg\n",
      "Moire image: 0108_gt.jpg\n",
      "Moire image: 0108_moire.jpg\n",
      "Moire image: 0109_gt.jpg\n",
      "Moire image: 0109_moire.jpg\n",
      "Moire image: 0110_gt.jpg\n",
      "Moire image: 0110_moire.jpg\n",
      "Moire image: 0111_gt.jpg\n",
      "Moire image: 0111_moire.jpg\n",
      "Moire image: 0112_gt.jpg\n",
      "Moire image: 0112_moire.jpg\n",
      "Moire image: 0114_gt.jpg\n",
      "Moire image: 0114_moire.jpg\n",
      "Moire image: 0115_gt.jpg\n",
      "Moire image: 0115_moire.jpg\n",
      "Moire image: 0116_gt.jpg\n",
      "Moire image: 0116_moire.jpg\n",
      "Moire image: 0117_gt.jpg\n",
      "Moire image: 0117_moire.jpg\n",
      "Moire image: 0118_gt.jpg\n",
      "Moire image: 0118_moire.jpg\n",
      "Moire image: 0119_gt.jpg\n",
      "Moire image: 0119_moire.jpg\n",
      "Moire image: 0120_gt.jpg\n",
      "Moire image: 0120_moire.jpg\n",
      "Moire image: 0121_gt.jpg\n",
      "Moire image: 0121_moire.jpg\n",
      "Moire image: 0122_gt.jpg\n",
      "Moire image: 0122_moire.jpg\n",
      "Moire image: 0123_gt.jpg\n",
      "Moire image: 0123_moire.jpg\n",
      "Moire image: 0124_gt.jpg\n",
      "Moire image: 0124_moire.jpg\n",
      "Moire image: 0125_gt.jpg\n",
      "Moire image: 0125_moire.jpg\n",
      "Moire image: 0127_gt.jpg\n",
      "Moire image: 0127_moire.jpg\n",
      "Moire image: 0128_gt.jpg\n",
      "Moire image: 0128_moire.jpg\n",
      "Moire image: 0129_gt.jpg\n",
      "Moire image: 0129_moire.jpg\n",
      "Moire image: 0131_gt.jpg\n",
      "Moire image: 0131_moire.jpg\n",
      "Moire image: 0133_gt.jpg\n",
      "Moire image: 0133_moire.jpg\n",
      "Moire image: 0134_gt.jpg\n",
      "Moire image: 0134_moire.jpg\n",
      "Moire image: 0135_gt.jpg\n",
      "Moire image: 0135_moire.jpg\n",
      "Moire image: 0137_gt.jpg\n",
      "Moire image: 0137_moire.jpg\n",
      "Moire image: 0139_gt.jpg\n",
      "Moire image: 0139_moire.jpg\n",
      "Moire image: 0140_gt.jpg\n",
      "Moire image: 0140_moire.jpg\n",
      "Moire image: 0141_gt.jpg\n",
      "Moire image: 0141_moire.jpg\n",
      "Moire image: 0142_gt.jpg\n",
      "Moire image: 0142_moire.jpg\n",
      "Moire image: 0143_gt.jpg\n",
      "Moire image: 0143_moire.jpg\n",
      "Moire image: 0144_gt.jpg\n",
      "Moire image: 0144_moire.jpg\n",
      "Moire image: 0145_gt.jpg\n",
      "Moire image: 0145_moire.jpg\n",
      "Moire image: 0146_gt.jpg\n",
      "Moire image: 0146_moire.jpg\n",
      "Moire image: 0147_gt.jpg\n",
      "Moire image: 0147_moire.jpg\n",
      "Moire image: 0148_gt.jpg\n",
      "Moire image: 0148_moire.jpg\n",
      "Moire image: 0149_gt.jpg\n",
      "Moire image: 0149_moire.jpg\n",
      "Moire image: 0150_gt.jpg\n",
      "Moire image: 0150_moire.jpg\n",
      "Moire image: 0151_gt.jpg\n",
      "Moire image: 0151_moire.jpg\n",
      "Moire image: 0152_gt.jpg\n",
      "Moire image: 0152_moire.jpg\n",
      "Moire image: 0153_gt.jpg\n",
      "Moire image: 0153_moire.jpg\n",
      "Moire image: 0154_gt.jpg\n",
      "Moire image: 0154_moire.jpg\n",
      "Moire image: 0155_gt.jpg\n",
      "Moire image: 0155_moire.jpg\n",
      "Moire image: 0157_gt.jpg\n",
      "Moire image: 0157_moire.jpg\n",
      "Moire image: 0158_gt.jpg\n",
      "Moire image: 0158_moire.jpg\n",
      "Moire image: 0159_gt.jpg\n",
      "Moire image: 0159_moire.jpg\n",
      "Moire image: 0160_gt.jpg\n",
      "Moire image: 0160_moire.jpg\n",
      "Moire image: 0162_gt.jpg\n",
      "Moire image: 0162_moire.jpg\n",
      "Moire image: 0163_gt.jpg\n",
      "Moire image: 0163_moire.jpg\n",
      "Moire image: 0164_gt.jpg\n",
      "Moire image: 0164_moire.jpg\n",
      "Moire image: 0165_gt.jpg\n",
      "Moire image: 0165_moire.jpg\n",
      "Moire image: 0166_gt.jpg\n",
      "Moire image: 0166_moire.jpg\n",
      "Moire image: 0167_gt.jpg\n",
      "Moire image: 0167_moire.jpg\n",
      "Moire image: 0168_gt.jpg\n",
      "Moire image: 0168_moire.jpg\n",
      "Moire image: 0169_gt.jpg\n",
      "Moire image: 0169_moire.jpg\n",
      "Moire image: 0170_gt.jpg\n",
      "Moire image: 0170_moire.jpg\n",
      "Moire image: 0171_gt.jpg\n",
      "Moire image: 0171_moire.jpg\n",
      "Moire image: 0172_gt.jpg\n",
      "Moire image: 0172_moire.jpg\n",
      "Moire image: 0174_gt.jpg\n",
      "Moire image: 0174_moire.jpg\n",
      "Moire image: 0175_gt.jpg\n",
      "Moire image: 0175_moire.jpg\n",
      "Moire image: 0176_gt.jpg\n",
      "Moire image: 0176_moire.jpg\n"
     ]
    }
   ],
   "source": [
    "# Directory where images are stored\n",
    "root_dir = 'Dataset/train/train/pair_00'  # Update this with the actual folder path\n",
    "\n",
    "# Create the dataset instance\n",
    "dataset = MoireCleanDataset(root_dir, transform=transform)\n",
    "\n",
    "# Print all the loaded images in the dataset\n",
    "for i in range(len(dataset)):\n",
    "    moire_img_name = dataset.images[i]\n",
    "    print(f\"Moire image: {moire_img_name}\")\n",
    "\n",
    "\n",
    "# Create the DataLoader instance for batching\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Using the GPU (Optional)\n",
    "If you have a GPU available, we can move the dataset, model, and tensors to the GPU to accelerate training.\n",
    "\n",
    "First, check if CUDA is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During data loading, make sure to move the tensors to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Debugging: Print the first few samples and labels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m moire_img, label, gt_img \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoire image shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, moire_img\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should print torch.Size([batch_size, 3, 256, 256])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean image shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, gt_img\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should print torch.Size([batch_size, 3, 256, 256])\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Debugging: Print the first few samples and labels\n",
    "for moire_img, label, gt_img in train_loader:\n",
    "    print(\"Moire image shape:\", moire_img.shape)  # Should print torch.Size([batch_size, 3, 256, 256])\n",
    "    print(\"Clean image shape:\", gt_img.shape)  # Should print torch.Size([batch_size, 3, 256, 256])\n",
    "    print(\"Labels:\", label)\n",
    "    break  # Only print the first batch\n",
    "\n",
    "\n",
    "# Iterate through the DataLoader and send data to GPU\n",
    "for moire_img, label, gt_img in train_loader:\n",
    "    # Move to GPU\n",
    "    moire_img, label, gt_img = moire_img.to(device), label.to(device), gt_img.to(device)\n",
    "    \n",
    "    # Now you can pass the images and labels through the model\n",
    "    print(f\"Batch of images shape: {moire_img.shape}\")\n",
    "    print(f\"Labels: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Training Loop (Example)\n",
    "Let’s assume you have a simple model to train on these images. Here’s an example of how to set up a basic model and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  80%|████████  | 8/10 [01:01<00:15,  7.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m     24\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m moire_img, label, gt_img \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     26\u001b[0m     moire_img, label \u001b[38;5;241m=\u001b[39m moire_img\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36mMoireDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     32\u001b[0m     moire_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(moire_img)\n\u001b[1;32m---> 33\u001b[0m     gt_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(gt_img)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Label: 1 for moire images, 0 for clean images\u001b[39;00m\n\u001b[0;32m     36\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Moire image\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a simple CNN model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),  # Convolutional layer\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # Pooling layer\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),  # Another convolution\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # Pooling layer\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 64 * 64, 1)  # Fully connected layer (you may need to adjust the dimensions)\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function (Binary Cross-Entropy for 2 classes) and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for moire_img, label, gt_img in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        moire_img, label = moire_img.to(device), label.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(moire_img)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs.squeeze(), label.float())  # Make sure the label is float for BCE\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Saving and Loading the Model\n",
    "Once the model is trained, you can save it and load it again later for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=131072, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'moire_model.pth')\n",
    "\n",
    "# Load the model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 64 * 64, 1)\n",
    ")\n",
    "model.load_state_dict(torch.load('moire_model.pth'))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "In this notebook guide:\n",
    "\n",
    "You’ve learned how to create a custom dataset in PyTorch to load pairs of images (moiré and clean).\n",
    "You’ve applied transformations to preprocess the images.\n",
    "You’ve utilized the GPU for faster training.\n",
    "You’ve set up a simple CNN model, a training loop, and demonstrated how to save/load models.\n",
    "This setup allows you to train a model that can classify whether an image contains moiré or is clean based on its corresponding pair. Adjust the network architecture, batch size, and other parameters as needed for your specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
